{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import math\n",
    "import cmath\n",
    "import re\n",
    "import os\n",
    "import glob\n",
    "from IPython import display\n",
    "from matplotlib import cm\n",
    "from matplotlib import gridspec\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import log_loss, accuracy_score, confusion_matrix\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.data import Dataset\n",
    "import seaborn as sns\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "pd.options.display.max_rows = 10\n",
    "pd.options.display.float_format = '{:.1f}'.format\n",
    "\n",
    "# DEBUGGING AND OUTPUT\n",
    "PLOT = True\n",
    "SUMMARIZE = True\n",
    "TEST = False\n",
    "\n",
    "# NOTES\n",
    "NOTES = \"mobilenetv2-96\"\n",
    "\n",
    "# Vars\n",
    "target_label = \"label\"\n",
    "id_label = \"ImageId\"\n",
    "TYPE = \"CNN\"\n",
    "OUTSTR = \"A{:.4f}_Type{}_LR{}_S{}_B{}{}.csv\"\n",
    "if TEST:\n",
    "    LEARNING_STEPS = 100\n",
    "    LEARNING_RATE = 0.01\n",
    "    BATCH_SIZE = 10\n",
    "else:\n",
    "    LEARNING_STEPS = 250\n",
    "    LEARNING_RATE = 0.01\n",
    "    BATCH_SIZE = 100\n",
    "\n",
    "# PATHS\n",
    "paths = {\n",
    "    \"Training\":\"/home/wesleytocs/.kaggle/competitions/digit-recognizer/train.csv\",\n",
    "    \"Testing\":\"/home/wesleytocs/.kaggle/competitions/digit-recognizer/test.csv\",\n",
    "    \"Submission\":\"/home/wesleytocs/.kaggle/competitions/digit-recognizer/sample_submission.csv\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def adjust_image(data):\n",
    "    # Reshape to [batch, height, width, channels].\n",
    "    imgs = tf.reshape(data, [-1, 28, 28, 1])\n",
    "    # Adjust image size to Inception-v3 input.\n",
    "    imgs = tf.image.resize_images(imgs, (96, 96))\n",
    "    # Convert to RGB image.\n",
    "    imgs = tf.image.grayscale_to_rgb(imgs)\n",
    "    return imgs\n",
    "\n",
    "def display(img, w, h):\n",
    "    one_image = img.reshape(w,h)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(one_image, cmap=cm.binary)\n",
    "    plt.show()\n",
    "\n",
    "def mobilenet_model_fn(features, labels, mode):\n",
    "    # Load mobilenet-v2 model.\n",
    "    module = hub.Module(\"https://tfhub.dev/google/imagenet/mobilenet_v2_035_96/classification/1\")\n",
    "    input_layer = adjust_image(features[\"x\"])\n",
    "    outputs = module(input_layer)\n",
    "\n",
    "    logits = tf.layers.dense(inputs=outputs, units=10)\n",
    "\n",
    "    predictions = {\n",
    "        # Generate predictions (for PREDICT and EVAL mode)\n",
    "        \"classes\": tf.argmax(input=logits, axis=1),\n",
    "        # Add `softmax_tensor` to the graph. It is used for PREDICT and by the\n",
    "        # `logging_hook`.\n",
    "        \"probabilities\": tf.nn.softmax(logits, name=\"softmax_tensor\")\n",
    "    }\n",
    "\n",
    "    if mode == tf.estimator.ModeKeys.PREDICT:\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, predictions=predictions)\n",
    "\n",
    "    # Calculate Loss (for both TRAIN and EVAL modes)\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "    predictions[\"loss\"] = loss\n",
    "\n",
    "    # Configure the Training Op (for TRAIN mode)\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        optimizer = tf.train.GradientDescentOptimizer(learning_rate=LEARNING_RATE)\n",
    "        train_op = optimizer.minimize(\n",
    "            loss=loss,\n",
    "            global_step=tf.train.get_global_step())\n",
    "        return tf.estimator.EstimatorSpec(mode=mode, loss=loss, train_op=train_op)\n",
    "\n",
    "    # Add evaluation metrics (for EVAL mode)\n",
    "    eval_metric_ops = {\n",
    "        \"accuracy\": tf.metrics.accuracy(\n",
    "            labels=labels, predictions=predictions[\"classes\"])}\n",
    "    return tf.estimator.EstimatorSpec(\n",
    "        mode=mode, loss=loss, eval_metric_ops=eval_metric_ops)\n",
    "\n",
    "def train_nn_classification_model(\n",
    "    learning_rate,\n",
    "    steps,\n",
    "    batch_size,\n",
    "    hidden_units,\n",
    "    training_examples,\n",
    "    training_targets,\n",
    "    validation_examples,\n",
    "    validation_targets):\n",
    "    \n",
    "    periods = steps // 100\n",
    "    steps_per_period = steps // periods  \n",
    "    \n",
    "    predict_training_input_fn = create_predict_input_fn(\n",
    "        training_examples, training_targets, batch_size)\n",
    "    predict_validation_input_fn = create_predict_input_fn(\n",
    "        validation_examples, validation_targets, batch_size)\n",
    "    training_input_fn = create_training_input_fn(\n",
    "        training_examples, training_targets, batch_size)\n",
    "    \n",
    "    my_optimizer = tf.train.AdagradOptimizer(learning_rate=learning_rate)\n",
    "    my_optimizer = tf.contrib.estimator.clip_gradients_by_norm(my_optimizer, 5.0)\n",
    "    classifier = tf.estimator.DNNClassifier(\n",
    "        feature_columns=construct_feature_columns(),\n",
    "        n_classes=10,\n",
    "        hidden_units = hidden_units,\n",
    "        optimizer=my_optimizer,\n",
    "        config=tf.estimator.RunConfig(keep_checkpoint_max=1)\n",
    "    )\n",
    "\n",
    "    # Train the model, but do so inside a loop so that we can periodically assess\n",
    "    # loss metrics.\n",
    "    print(\"Training model...\\nMetrics:\")\n",
    "    print(\"\\tPERIOD\\tTYPE\\tTRAIN.\\tVALID.\")\n",
    "    training_accuracies = []\n",
    "    validation_accuracies = []\n",
    "    training_errors = []\n",
    "    validation_errors = []\n",
    "    for period in range (0, periods):\n",
    "        # Train the model, starting from the prior state.\n",
    "        classifier.train(\n",
    "            input_fn=training_input_fn,\n",
    "            steps=steps_per_period\n",
    "        )\n",
    "        # Take a break and compute probabilities.\n",
    "        training_predictions = list(classifier.predict(input_fn=predict_training_input_fn))\n",
    "        training_probabilities = np.array([item['probabilities'] for item in training_predictions])\n",
    "        training_pred_class_id = np.array([item['class_ids'][0] for item in training_predictions])\n",
    "        training_pred_one_hot = tf.keras.utils.to_categorical(training_pred_class_id,10)\n",
    "        training_targets_one_hot = tf.keras.utils.to_categorical(training_targets,10)\n",
    "\n",
    "        validation_predictions = list(classifier.predict(input_fn=predict_validation_input_fn))\n",
    "        validation_probabilities = np.array([item['probabilities'] for item in validation_predictions])    \n",
    "        validation_pred_class_id = np.array([item['class_ids'][0] for item in validation_predictions])\n",
    "        validation_pred_one_hot = tf.keras.utils.to_categorical(validation_pred_class_id,10)  \n",
    "        validation_targets_one_hot = tf.keras.utils.to_categorical(validation_targets,10)  \n",
    "\n",
    "        # Compute training and validation errors.\n",
    "        training_log_loss = log_loss(training_targets, training_pred_one_hot)\n",
    "        validation_log_loss = log_loss(validation_targets, validation_pred_one_hot)\n",
    "        training_accuracy = 100 * accuracy_score(training_targets_one_hot, training_pred_one_hot)\n",
    "        validation_accuracy = 100 * accuracy_score(validation_targets_one_hot, validation_pred_one_hot)\n",
    "        # Occasionally print the current loss.\n",
    "        print(\"\\t{}\\tLgLs\\t{:.2f}\\t{:.2f}\".format(period, training_log_loss, validation_log_loss))\n",
    "        print(\"\\t\\tAcc.\\t{:.2f}%\\t{:.2f}%\\n\".format(training_accuracy, validation_accuracy))\n",
    "        # Add the loss metrics from this period to our list.\n",
    "        training_errors.append(training_log_loss)\n",
    "        validation_errors.append(validation_log_loss)\n",
    "        training_accuracies.append(training_accuracy)\n",
    "        validation_accuracies.append(validation_accuracy)\n",
    "    print(\"Model training finished.\")\n",
    "    # Remove event files to save disk space.\n",
    "    _ = map(os.remove, glob.glob(os.path.join(classifier.model_dir, 'events.out.tfevents*')))\n",
    "\n",
    "    # Calculate final predictions (not probabilities, as above).\n",
    "    final_predictions = classifier.predict(input_fn=predict_validation_input_fn)\n",
    "    final_predictions = np.array([item['class_ids'][0] for item in final_predictions])\n",
    "\n",
    "\n",
    "    accuracy = 100 * accuracy_score(validation_targets, final_predictions)\n",
    "    print(\"Final accuracy (on validation data): {:.4f}%\".format(accuracy))\n",
    "\n",
    "    # Output a graph of loss metrics over periods.\n",
    "    plt.ylabel(\"LogLoss\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"LogLoss vs. Periods\")\n",
    "    plt.plot(training_errors, label=\"training\")\n",
    "    plt.plot(validation_errors, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    # Output a graph of loss metrics over periods.\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.xlabel(\"Periods\")\n",
    "    plt.title(\"Accuracy vs. Periods\")\n",
    "    plt.plot(training_accuracies, label=\"training\")\n",
    "    plt.plot(validation_accuracies, label=\"validation\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Output a plot of the confusion matrix.\n",
    "    cm = confusion_matrix(validation_targets, final_predictions)\n",
    "    # Normalize the confusion matrix by row (i.e by the number of samples\n",
    "    # in each class).\n",
    "    cm_normalized = cm.astype(\"float\") / cm.sum(axis=1)[:, np.newaxis]\n",
    "    ax = sns.heatmap(cm_normalized, cmap=\"bone_r\")\n",
    "    ax.set_aspect(1)\n",
    "    plt.title(\"Confusion matrix\")\n",
    "    plt.ylabel(\"True label\")\n",
    "    plt.xlabel(\"Predicted label\")\n",
    "    plt.show()\n",
    "    return (classifier, accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY8AAAEWCAYAAACe8xtsAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEkegMNIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f608eeab400>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = pd.read_csv(paths[\"Training\"], sep=\",\").astype(np.int32)\n",
    "if TEST:\n",
    "    train_data = train_data.head(1000)\n",
    "train_data = train_data.reindex(np.random.permutation(train_data.index))\n",
    "ax = sns.countplot(x=\"label\", data=train_data)\n",
    "ax.set_title(\"Digit Counts\")\n",
    "train_labels = np.asarray(train_data.pop(target_label), dtype=np.int32)\n",
    "train_data = train_data / 255\n",
    "train_data = np.asarray(train_data, dtype=np.float32)\n",
    "\n",
    "t_size = int(0.8 * train_data.shape[0])\n",
    "\n",
    "validation_data = train_data[t_size:]\n",
    "validation_labels = train_labels[t_size:]\n",
    "train_data = train_data[:t_size]\n",
    "train_labels = train_labels[:t_size]\n",
    "\n",
    "eval_data = pd.read_csv(paths[\"Testing\"], sep=\",\")\n",
    "eval_labels = pd.read_csv(paths[\"Submission\"], sep=\",\")[\"Label\"]\n",
    "if TEST:\n",
    "    eval_data = eval_data.head(1000)\n",
    "    eval_labels = eval_labels.head(1000)\n",
    "eval_data = eval_data / 255\n",
    "eval_data = np.asarray(eval_data, dtype=np.float32)\n",
    "eval_labels = np.asarray(eval_labels, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "(33600, 784)\n",
      "(33600,)\n",
      "----------------------------------------\n",
      "VALIDATION\n",
      "(8400, 784)\n",
      "(8400,)\n",
      "----------------------------------------\n",
      "EVALUATION\n",
      "(28000, 784)\n",
      "(28000,)\n",
      "{0}\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(train_data.shape)\n",
    "print(train_labels.shape)\n",
    "print(\"-\" * 40)\n",
    "print(\"VALIDATION\")\n",
    "print(validation_data.shape)\n",
    "print(validation_labels.shape)\n",
    "print(\"-\" * 40)\n",
    "print(\"EVALUATION\")\n",
    "print(eval_data.shape)\n",
    "print(eval_labels.shape)\n",
    "print(set(eval_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the Estimator\n",
    "# classifier = tf.estimator.Estimator(\n",
    "#     model_fn=mobilenet_model_fn, model_dir=\"/tmp/convnet_model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train the model\n",
    "# train_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#     x={\"x\": train_data},\n",
    "#     y=train_labels,\n",
    "#     batch_size=BATCH_SIZE,\n",
    "#     num_epochs=None,\n",
    "#     shuffle=True)\n",
    "\n",
    "# training_eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#     x={\"x\": train_data},\n",
    "#     y=train_labels,\n",
    "#     num_epochs=1,\n",
    "#     shuffle=False)\n",
    "\n",
    "# validation_eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "#     x={\"x\": validation_data},\n",
    "#     y=validation_labels,\n",
    "#     num_epochs=1,\n",
    "#     shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nearest_2' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-49b384f370fc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnearest_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_size\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munitstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-49b384f370fc>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mlayers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0munits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mnearest_2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mceil\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_size\u001b[0m \u001b[0;34m**\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0munitstr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"_\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0munits\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nearest_2' is not defined"
     ]
    }
   ],
   "source": [
    "layers = 3\n",
    "units = [nearest_2(math.ceil(t_size ** (1/i))) for i in range(2, 2+layers)]\n",
    "print(units)\n",
    "unitstr = \"_\".join([str(x) for x in units])\n",
    "\n",
    "classifier, accuracy = train_nn_classification_model(\n",
    "    learning_rate=LEARNING_RATE,\n",
    "    steps=LEARNING_STEPS,\n",
    "    hidden_units=units,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    training_examples=training_examples,\n",
    "    training_targets=training_targets,\n",
    "    validation_examples=validation_examples,\n",
    "    validation_targets=validation_targets)\n",
    "print(\"Final accuracy on validation set: {:.2f}%\".format(accuracy * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#tensors_to_log = {\"loss\": loss, \"accuracy\" : accuracy}\n",
    "# logging_hook = tf.train.LoggingTensorHook({\"loss\" : \"loss\"}, every_n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# periods = LEARNING_STEPS // 5\n",
    "# steps_per_period = LEARNING_STEPS // periods\n",
    "# t_accs = []\n",
    "# v_accs = []\n",
    "# t_lls = []\n",
    "# v_lls = []\n",
    "# print(\"Training model...\\nMetrics:\")\n",
    "# print(\"\\tPERIOD\\tTYPE\\tTRAIN.\\tVALID.\")\n",
    "# for period in range(periods):\n",
    "# classifier.train(\n",
    "#     input_fn=train_input_fn,\n",
    "#     steps=LEARNING_STEPS,\n",
    "#     hooks=[logging_hook])\n",
    "#     training_stats = classifier.evaluate(input_fn=training_eval_input_fn)\n",
    "#     validation_stats = classifier.evaluate(input_fn=validation_eval_input_fn)\n",
    "#     t_ll = training_stats[\"loss\"]\n",
    "#     t_acc = 100 * training_stats[\"accuracy\"]\n",
    "#     v_ll = validation_stats[\"loss\"]\n",
    "#     v_acc = 100 * validation_stats[\"accuracy\"]\n",
    "#     print(\"\\t{}\\tLgLs\\t{:.2f}\\t{:.2f}\".format(period, t_ll, v_ll))\n",
    "#     print(\"\\t\\tAcc.\\t{:.2f}%\\t{:.2f}%\\n\".format(t_acc, v_acc))\n",
    "#     t_lls.append(t_ll);\n",
    "#     v_lls.append(v_ll)\n",
    "#     t_accs.append(t_acc)\n",
    "#     v_accs.append(v_acc)\n",
    "# accuracy = classifier.evaluate(input_fn=validation_eval_input_fn)[\"accuracy\"] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy (on validation data): 93.2262%\n"
     ]
    }
   ],
   "source": [
    "print(\"Final accuracy (on validation data): {:.4f}%\".format(accuracy))\n",
    "\n",
    "# # Output a graph of loss metrics over periods.\n",
    "# plt.ylabel(\"LogLoss\")\n",
    "# plt.xlabel(\"Periods\")\n",
    "# plt.title(\"LogLoss vs. Periods\")\n",
    "# plt.plot(t_lls, label=\"training\")\n",
    "# plt.plot(v_lls, label=\"validation\")\n",
    "# plt.legend()\n",
    "# plt.show()\n",
    "\n",
    "# # Output a graph of accuracy over periods.\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.xlabel(\"Periods\")\n",
    "# plt.title(\"Accuracy vs. Periods\")\n",
    "# plt.plot(t_accs, label=\"training\")\n",
    "# plt.plot(v_accs, label=\"validation\")\n",
    "# plt.legend()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on evaluation data (expected ~10%): 9.91%\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the model and print results\n",
    "eval_input_fn = tf.estimator.inputs.numpy_input_fn(\n",
    "    x={\"x\": eval_data},\n",
    "    y=eval_labels,\n",
    "    num_epochs=1,\n",
    "    shuffle=False)\n",
    "stats = classifier.evaluate(input_fn=eval_input_fn)\n",
    "t_accuracy = 100 * stats[\"accuracy\"]\n",
    "print(\"Accuracy on evaluation data (expected ~10%): {:.2f}%\".format(t_accuracy))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Make Predictions and output the results\n",
    "predictions = np.array([item['classes'] for item in classifier.predict(input_fn=eval_input_fn)])\n",
    "ids = np.array([i + 1 for i in range(len(predictions))])\n",
    "output = pd.DataFrame({id_label:ids, target_label:predictions}, dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Outputting to: 'A93.2262_TypeCNN_LR0.01_S250_B100_mobilenetv2-96.csv'\n"
     ]
    }
   ],
   "source": [
    "if not TEST:\n",
    "    name = OUTSTR.format(accuracy, TYPE, LEARNING_RATE, LEARNING_STEPS, BATCH_SIZE,\"_\" + NOTES if NOTES else \"\")\n",
    "    print(\"Outputting to: '{}'\".format(name))\n",
    "    output.to_csv(name, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
